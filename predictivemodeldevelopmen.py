# -*- coding: utf-8 -*-
"""PredictiveModelDevelopmen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CU0ZAJ0pcUj9m-wA55AHDI3PK6R-rK_N

# WIE3007 Data Mining Project

Predictive Model Development (Logistic Regression)
*   Name : Nasrin Ameera Binti Mohammad Zabri
*   Matrics Num : 22002363


Predictive Model Development (Random Forest)
*   Name : Fatin Aina binti Nor Ikhsan
*   Matrics Num : 22001278

1. Import required libraries
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve

import matplotlib.pyplot as plt
import seaborn as sns

"""2. Load model-ready dataset"""

X = pd.read_csv("X_features.csv")
y = pd.read_csv("y_target.csv")

print(X.shape)
print(y.value_counts())

"""3. Train-test Split"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

"""4. Feature Scaling"""

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Logistic Regression

Train Logistic Regression Model
"""

log_reg = LogisticRegression(
    max_iter=1000,
    class_weight="balanced",
    random_state=42
)

log_reg.fit(X_train_scaled, y_train)

"""Model Predictions"""

y_pred = log_reg.predict(X_test_scaled)
y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]

"""Model Evaluation Metrics (Accuracy, F1, ROC-AUC)"""

accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)

print(f"Accuracy: {accuracy:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"ROC-AUC: {roc_auc:.4f}")

"""Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

"""ROC Curve"""

fpr, tpr, _ = roc_curve(y_test, y_prob)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"ROC-AUC = {roc_auc:.3f}")
plt.plot([0,1], [0,1], linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Logistic Regression")
plt.legend()
plt.show()

"""Feature Importance"""

feature_importance = pd.DataFrame({
    "Feature": X.columns,
    "Coefficient": log_reg.coef_[0]
})

feature_importance["Abs_Coefficient"] = feature_importance["Coefficient"].abs()
feature_importance = feature_importance.sort_values(
    by="Abs_Coefficient",
    ascending=False
)

feature_importance.head(10)

"""*Plot Top Features"""

plt.figure(figsize=(8,5))
sns.barplot(
    data=feature_importance.head(10),
    x="Coefficient",
    y="Feature"
)
plt.title("Top 10 Features Influencing Churn")
plt.show()

"""**Model Interpretation**

Logistic Regression provides interpretable coefficients indicating the direction and strength
of each feature’s influence on churn. Features with higher absolute coefficients have stronger
predictive power. Positive coefficients increase churn likelihood, while negative coefficients
are associated with customer retention.

# Random Forest
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score

"""Train Random Forest Model"""

rf_model = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    max_depth=None
)

rf_model.fit(X_train, y_train)

"""Model Predictions"""

y_pred_rf = rf_model.predict(X_test)
y_prob_rf = rf_model.predict_proba(X_test)[:, 1]

"""Model Evaluation Metrics (Accuracy, F1, ROC-AUC)



"""

accuracy_rf = accuracy_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
roc_auc_rf = roc_auc_score(y_test, y_prob_rf)

print(f"Accuracy: {accuracy_rf:.4f}")
print(f"F1-score: {f1_rf:.4f}")
print(f"ROC-AUC: {roc_auc_rf:.4f}")

"""Confusion Matrix



"""

cm = confusion_matrix(y_test, y_pred_rf)

plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Random Forest")
plt.show()

"""ROC Curve"""

fpr, tpr, _ = roc_curve(y_test, y_prob_rf)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"ROC-AUC = {roc_auc_rf:.3f}")
plt.plot([0,1], [0,1], linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Random Forest")
plt.legend()
plt.show()

"""Feature Importance"""

rf_feature_importance = pd.DataFrame({
    "Feature": X.columns,
    "Importance": rf_model.feature_importances_
})

rf_feature_importance = rf_feature_importance.sort_values(
    by="Importance",
    ascending=False
)

rf_feature_importance.head(10)

"""*Plot Top Features"""

plt.figure(figsize=(8,5))
sns.barplot(
    data=rf_feature_importance.head(10),
    x="Importance",
    y="Feature"
)
plt.title("Top 10 Features Influencing Churn")
plt.show()

"""**Model Interpretation**

Random Forest evaluates feature importance based on how much each feature contributes to reducing impurity across multiple decision trees. Features with higher importance scores play a more significant role in the model’s decision-making process. Unlike Logistic Regression, Random Forest does not provide directional effects but is able to capture non-linear relationships and interactions between features, leading to stronger predictive performance.

# Model Comparison
"""

comparison_df = pd.DataFrame({
    "Model": ["Logistic Regression", "Random Forest"],
    "Accuracy": [accuracy, accuracy_rf],
    "F1-score": [f1, f1_rf],
    "ROC-AUC": [roc_auc, roc_auc_rf]
})

comparison_df

"""**Model Comparison Interpretation**

Both models demonstrate strong predictive performance. Logistic Regression achieved an accuracy of 0.9958, an F1-score of 0.9909, and a ROC-AUC of 0.9999, indicating that it is highly effective in predicting churn while maintaining good balance between precision and recall.

Random Forest slightly outperforms Logistic Regression, achieving perfect scores across all evaluation metrics. This suggests that Random Forest is better able to capture complex, non-linear relationships and feature interactions present in the data, leading to improved classification performance.

Overall, while Logistic Regression provides better interpretability through its coefficients, Random Forest offers superior predictive accuracy, making it the more suitable model for this task.
"""
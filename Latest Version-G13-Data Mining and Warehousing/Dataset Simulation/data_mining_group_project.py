# -*- coding: utf-8 -*-
"""Data Mining Group Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kUlCsnHD9LTxZ57Gu_Bed_0EpE-Gj62j
"""

!pip install -q pandas numpy scikit-learn matplotlib transformers torch xgboost shap

!pip install -q -U google-genai

import os
os.environ["GOOGLE_API_KEY"] = "your_API_key_here"

import numpy as np
import pandas as pd
import json
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, learning_curve
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix

from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

from sklearn.inspection import permutation_importance
from xgboost import XGBClassifier

"""# DATASET GENERATION"""

RANDOM_SEED = 42
rng = np.random.default_rng(RANDOM_SEED)

# -----------------------
# 1) Simulate base dataset
# -----------------------
def clip(a, lo, hi):
    return np.clip(a, lo, hi)

N = 1500  # >= 1000
states = ["Selangor", "KL", "Johor", "Penang", "Perak", "Sabah", "Sarawak", "Pahang", "Kelantan", "Terengganu"]
income_bands = ["B40", "M40", "T20"]
segments = ["new", "regular", "loyal", "premium"]
genders = ["F", "M"]

df = pd.DataFrame({
    "customer_id": [f"C{100000+i}" for i in range(N)],
    "age": clip(rng.normal(30, 9, N).round(), 18, 65).astype(int),
    "gender": rng.choice(genders, N, p=[0.55, 0.45]),
    "state": rng.choice(states, N, p=[0.22, 0.14, 0.12, 0.08, 0.08, 0.07, 0.07, 0.08, 0.07, 0.07]),
    "income_band": rng.choice(income_bands, N, p=[0.48, 0.40, 0.12]),
    "tenure_days": clip(rng.gamma(shape=2.0, scale=120, size=N).round(), 7, 1500).astype(int),
    "segment": rng.choice(segments, N, p=[0.25, 0.45, 0.22, 0.08]),
})

# A multiplier that adjusts how frequently a customer places orders, based on their segment
seg_mult_orders = df["segment"].map({"new": 0.6, "regular": 1.0, "loyal": 1.5, "premium": 1.8}).values

# A multiplier for Average Order Value (AOV) based on customer segment.
seg_mult_aov = df["segment"].map({"new": 0.9, "regular": 1.0, "loyal": 1.1, "premium": 1.3}).values

# Number of orders placed in the last 90 days.
orders_90d = clip(rng.poisson(lam=3.5 * seg_mult_orders), 0, 40).astype(int)

# Number of orders in the last 30 days, derived from recent activity.
orders_30d = clip((orders_90d * rng.uniform(0.15, 0.65, N)).round(), 0, 20).astype(int)

# Average monetary value per order (RM).
avg_order_value = clip(rng.lognormal(mean=3.7, sigma=0.35, size=N) * seg_mult_aov, 15, 600).round(2)

# Total customer spending in the last 90 days. (with random noise)
total_spend_90d = (orders_90d * avg_order_value * rng.uniform(0.8, 1.2, N)).round(2)

# Number of days since the customer’s most recent purchase.
days_since_last_order = rng.integers(0, 120, N)
days_since_last_order = clip(days_since_last_order - (orders_30d * rng.integers(0, 3, N)), 0, 120).astype(int)

# Number of browsing sessions in the last 30 days.
browse_sessions_30d = clip(rng.poisson(lam=10 * seg_mult_orders) + rng.integers(0, 10, N), 0, 120).astype(int)

# Proportion of shopping carts abandoned before checkout.
cart_abandon_rate = clip(rng.beta(a=2.0, b=5.0, size=N) + (days_since_last_order / 300), 0, 0.98).round(3)

# Proportion of orders returned by the customer.
return_rate = clip(rng.beta(a=1.3, b=15, size=N) + rng.normal(0, 0.01, N), 0, 0.60).round(3)

# Number of refunds requested in the last 90 days.
refund_count_90d = clip(rng.poisson(lam=orders_90d * return_rate * 0.7), 0, 20).astype(int)

# Proportion of purchases made using discounts.
discount_ratio = clip(
    rng.beta(a=2.2, b=4.5, size=N) + (df["income_band"].map({"B40": 0.08, "M40": 0.03, "T20": -0.02}).values),
    0, 0.95
).round(3)

# Frequency of coupon usage across purchases.
coupon_used_rate = clip(rng.beta(a=2.0, b=3.0, size=N) + (discount_ratio - 0.3), 0, 1.0).round(3)

# Proportion of orders delivered later than expected.
delivery_delay_rate = clip(rng.beta(a=1.5, b=12.0, size=N) + rng.normal(0, 0.02, N), 0, 0.70).round(3)

# Number of delayed deliveries in the last 90 days.
late_delivery_count = clip(rng.poisson(lam=orders_90d * delivery_delay_rate * 0.6), 0, 15).astype(int)

# Number of completely failed deliveries.
failed_delivery_count = clip(rng.poisson(lam=orders_90d * 0.01), 0, 5).astype(int)

# Number of customer support tickets raised in the last 90 days.
tickets_90d = clip(rng.poisson(lam=0.3 + (late_delivery_count * 0.15) + (refund_count_90d * 0.12)), 0, 12).astype(int)

# Average time (in hours) taken for customer support to respond.
avg_first_response_hours = clip(rng.lognormal(mean=2.0, sigma=0.5, size=N) + tickets_90d * 0.8, 0.5, 72).round(2)

# Proportion of marketing emails opened by the customer.
email_open_rate = clip(rng.beta(a=2.5, b=3.5, size=N) - (days_since_last_order / 250), 0, 1.0).round(3)

# Proportion of mobile push notifications opened.
push_open_rate = clip(rng.beta(a=2.0, b=4.0, size=N) - (days_since_last_order / 300), 0, 1.0).round(3)

# Accumulated loyalty points held by the customer.
loyalty_points_balance = clip((total_spend_90d * rng.uniform(0.3, 1.2, N)).round(), 0, 50000).astype(int)

# Customer’s rating of the mobile app (1–5).
app_rating = clip(rng.normal(4.1, 0.7, N) - (late_delivery_count * 0.08) - (tickets_90d * 0.06), 1, 5).round(1)

df = df.assign(
    orders_90d=orders_90d,
    orders_30d=orders_30d,
    avg_order_value=avg_order_value,
    total_spend_90d=total_spend_90d,
    days_since_last_order=days_since_last_order,
    browse_sessions_30d=browse_sessions_30d,
    cart_abandon_rate=cart_abandon_rate,
    return_rate=return_rate,
    refund_count_90d=refund_count_90d,
    discount_ratio=discount_ratio,
    coupon_used_rate=coupon_used_rate,
    delivery_delay_rate=delivery_delay_rate,
    late_delivery_count=late_delivery_count,
    failed_delivery_count=failed_delivery_count,
    tickets_90d=tickets_90d,
    avg_first_response_hours=avg_first_response_hours,
    email_open_rate=email_open_rate,
    push_open_rate=push_open_rate,
    loyalty_points_balance=loyalty_points_balance,
    app_rating=app_rating
)

# Generating Churn Label

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

risk = (
    0.035 * df["days_since_last_order"]
    + 0.9 * df["delivery_delay_rate"] * 10
    + 0.45 * df["refund_count_90d"]
    + 0.35 * df["tickets_90d"]
    + 0.25 * df["cart_abandon_rate"] * 10
    - 0.30 * df["orders_30d"]
    - 0.18 * df["email_open_rate"] * 10
    - 0.15 * df["push_open_rate"] * 10
    - 0.00002 * df["loyalty_points_balance"]
    - 0.40 * (df["app_rating"] - 3.0)
)

p = sigmoid((risk - np.percentile(risk, 55)) / 2.2)
df["churn"] = (rng.uniform(0, 1, N) < p).astype(int)

print("Rows:", len(df))
print("Churn rate:", round(df["churn"].mean(), 3))

# Realistic attempt (injecting missing values)

for col, miss_rate in {
    "income_band": 0.03,
    "avg_first_response_hours": 0.04,
    "app_rating": 0.02,
    "discount_ratio": 0.03,
}.items():
    m = rng.uniform(0, 1, N) < miss_rate
    df.loc[m, col] = np.nan

#LLM used for feedbacks

import json, time, re, os
from google import genai

client = genai.Client()  # uses GOOGLE_API_KEY from environment

INPUT_PATH = "llm_text_generation_input.jsonl"
OUTPUT_PATH = "llm_text_generation_output.jsonl"

# LLM generate feedback text
TEXT_PROMPT = """
You are generating realistic Malaysian e-commerce customer text.
Given the customer profile and behaviour summary, output a JSON object with keys:
- customer_feedback: 1-2 sentences (casual English with occasional Malay, realistic typos ok)
- support_chat_excerpt: 1-2 lines like a chat message (customer side)
- reason_for_low_activity: short phrase
If delivery delay high -> complain about shipping.
If refund high -> complain about product quality/refund.
If churn=0 -> more positive/neutral.
Return ONLY valid JSON with the 3 keys above, no extra commentary.
""".strip()

def build_text_input_row(r):
    return {
        "customer_id": r["customer_id"],
        "segment": r["segment"],
        "income_band": r["income_band"] if pd.notna(r["income_band"]) else "unknown",
        "orders_30d": int(r["orders_30d"]),
        "days_since_last_order": int(r["days_since_last_order"]),
        "late_delivery_count": int(r["late_delivery_count"]),
        "refund_count_90d": int(r["refund_count_90d"]),
        "discount_ratio": float(r["discount_ratio"]) if pd.notna(r["discount_ratio"]) else None,
        "app_rating": float(r["app_rating"]) if pd.notna(r["app_rating"]) else None,
        "churn": int(r["churn"]),
    }

#just to clear any errors of previous attempts
if os.path.exists(OUTPUT_PATH):
    os.remove(OUTPUT_PATH)

# 1) Write INPUT JSONL (prompts)

N_TEXT_ROWS = 500

with open(INPUT_PATH, "w", encoding="utf-8") as f:
    for _, r in df.sample(N_TEXT_ROWS, random_state=RANDOM_SEED).iterrows():
        f.write(json.dumps({"prompt": TEXT_PROMPT, "input": build_text_input_row(r)}, ensure_ascii=False) + "\n")

print("Wrote:", INPUT_PATH, "| rows:", N_TEXT_ROWS)

def extract_json_anywhere(text: str) -> dict:
    """
    Gemini/Gemma sometimes wraps JSON in fences or adds extra text.
    This extracts the first {...} JSON object and parses it.
    """
    if text is None:
        raise ValueError("Empty response text")

    t = text.strip()

    # remove ```json fences if present
    t = re.sub(r"^```json\s*", "", t, flags=re.IGNORECASE)
    t = re.sub(r"^```\s*", "", t)
    t = re.sub(r"\s*```$", "", t)

    # try direct JSON parse first
    try:
        return json.loads(t)
    except Exception:
        pass

    # otherwise find the first JSON object inside the text
    m = re.search(r"\{.*\}", t, flags=re.DOTALL)
    if not m:
        raise ValueError("No JSON object found in response text")
    return json.loads(m.group(0))

def run_llm_text_generation(
    input_path=INPUT_PATH,
    output_path=OUTPUT_PATH,
    model="models/gemma-3-4b-it",
    base_sleep_s=2.2,          # ✅ ~27 req/min (stays under 30/min)
    max_retries_per_row=5
):
    fails = 0
    printed_first_error = False

    with open(input_path, "r", encoding="utf-8") as f_in, open(output_path, "w", encoding="utf-8") as f_out:
        for i, line in enumerate(f_in, start=1):
            obj = json.loads(line)
            prompt = obj["prompt"]
            inp = obj["input"]

            full_text = (
                prompt
                + "\n\nCustomer summary (JSON):\n"
                + json.dumps(inp, ensure_ascii=False)
            )

            attempt = 0
            while True:
                try:
                    resp = client.models.generate_content(model=model, contents=full_text)
                    parsed = extract_json_anywhere(resp.text)

                    out = {
                        "customer_id": inp["customer_id"],
                        "customer_feedback": parsed.get("customer_feedback", ""),
                        "support_chat_excerpt": parsed.get("support_chat_excerpt", ""),
                        "reason_for_low_activity": parsed.get("reason_for_low_activity", "")
                    }

                    f_out.write(json.dumps(out, ensure_ascii=False) + "\n")
                    break  # ✅ success -> next row

                except Exception as e:
                    msg = str(e)

                    # --- Handle 429 rate limit / quota exceeded ---
                    if "429" in msg or "RESOURCE_EXHAUSTED" in msg:
                        attempt += 1
                        if attempt > max_retries_per_row:
                            fails += 1
                            if not printed_first_error:
                                print("FIRST ERROR:", repr(e))
                                printed_first_error = True

                            out = {
                                "customer_id": inp["customer_id"],
                                "customer_feedback": "",
                                "support_chat_excerpt": "",
                                "reason_for_low_activity": ""
                            }
                            f_out.write(json.dumps(out, ensure_ascii=False) + "\n")
                            break

                        # Respect retry delay if present in error
                        wait_s = 30.0
                        m = re.search(r"retry in ([0-9.]+)s", msg)
                        if m:
                            wait_s = max(5.0, float(m.group(1)) + 2.0)
                        m2 = re.search(r"retryDelay.*?([0-9]+)s", msg)
                        if m2:
                            wait_s = max(wait_s, float(int(m2.group(1)) + 2))

                        print(f"[{i}/{N_TEXT_ROWS}] 429 rate limit. Sleeping {wait_s:.1f}s then retrying (attempt {attempt}/{max_retries_per_row})...")
                        time.sleep(wait_s)
                        continue

                    # --- Other errors: fallback ---
                    fails += 1
                    if not printed_first_error:
                        print("FIRST ERROR:", repr(e))
                        printed_first_error = True

                    out = {
                        "customer_id": inp["customer_id"],
                        "customer_feedback": "",
                        "support_chat_excerpt": "",
                        "reason_for_low_activity": ""
                    }
                    f_out.write(json.dumps(out, ensure_ascii=False) + "\n")
                    break

            # Base pacing (prevents hitting 30/min)
            time.sleep(base_sleep_s)

    print("Wrote:", output_path, "| fails:", fails)

# 2) Run LLM -> creates OUTPUT JSONL (now quota-safe)
run_llm_text_generation(model="models/gemma-3-4b-it", base_sleep_s=2.2)

# 3) Merge back into df so df shows text columns
df = merge_llm_text_outputs(df, OUTPUT_PATH)

# 4) Ensure no NaN
for c in ["customer_feedback", "support_chat_excerpt", "reason_for_low_activity"]:
    df[c] = df[c].fillna("")

# quick check
df[["customer_id", "customer_feedback", "support_chat_excerpt", "reason_for_low_activity"]].head(5)

import json
import pandas as pd

# Load LLM output
rows = []
with open("llm_text_generation_output.jsonl", "r", encoding="utf-8") as f:
    for line in f:
        rows.append(json.loads(line))

df_text = pd.DataFrame(rows)

# Merge into main df
df = df.merge(df_text, on="customer_id", how="left")

# Safety: ensure columns always exist
for c in ["customer_feedback", "support_chat_excerpt", "reason_for_low_activity"]:
    if c not in df.columns:
        df[c] = ""
    df[c] = df[c].fillna("")

print("Rows with feedback:", (df["customer_feedback"].str.strip() != "").sum())
df[["customer_id","customer_feedback","support_chat_excerpt","reason_for_low_activity"]].head(5)

with open("llm_text_generation_output.jsonl", "r", encoding="utf-8") as f:
    for _ in range(5):
        print(f.readline())

(df["customer_feedback"].str.strip() != "").sum(), len(df)

df.loc[df["customer_feedback"].str.strip() != "",
       ["customer_id","customer_feedback","support_chat_excerpt","reason_for_low_activity"]].head(10)

# Load output jsonl into df_text and check overlap
import pandas as pd, json

rows = []
with open("llm_text_generation_output.jsonl", "r", encoding="utf-8") as f:
    for line in f:
        rows.append(json.loads(line))

df_text = pd.DataFrame(rows)
print("Output rows:", len(df_text))
print("Unique output customer_id:", df_text["customer_id"].nunique())
print("Overlap with df:", df["customer_id"].isin(df_text["customer_id"]).sum())

df.to_csv("ecommerce_churn_llm_final.csv", index=False)
print("Final dataset saved.")

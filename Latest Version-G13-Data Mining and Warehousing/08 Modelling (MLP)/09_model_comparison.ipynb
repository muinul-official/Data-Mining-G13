{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 2: Advanced Model Comparison and Evaluation\n",
                "\n",
                "## Objective\n",
                "To scientifically evaluate and compare the performance of 5 different machine learning models (Decision Tree, Random Forest, Logistic Regression, MLP Neural Network, and XGBoost) on the E-Commerce Customer Churn dataset, utilizing advanced techniques like hyperparameter tuning and model explainability.\n",
                "\n",
                "## Methodology\n",
                "1.  **Modularized Source**: Use `src.preprocessing`, `src.data_preparation`, and `src.eda` for clean, reusable logic.\n",
                "2.  **Hyperparameter Tuning**: Use `GridSearchCV` to find optimal settings for each model.\n",
                "3.  **Cross-Validation**: Validate model stability using 5-fold cross-validation.\n",
                "4.  **SHAP Explainability**: Analyze feature importance using SHAP values for the best-performing model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup and Imports\n",
                "!pip install -q pandas numpy scikit-learn matplotlib seaborn transformers torch xgboost shap python-dotenv\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "import shap\n",
                "\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
                "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, f1_score, roc_auc_score, confusion_matrix, \n",
                "    classification_report, roc_curve, auc, precision_score, recall_score\n",
                ")\n",
                "\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.neural_network import MLPClassifier\n",
                "from xgboost import XGBClassifier\n",
                "\n",
                "# Import modular functions\n",
                "from src.preprocessing import clean_data, add_sentiment_features\n",
                "from src.eda import plot_churn_distribution, plot_correlation_heatmap\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "RANDOM_SEED = 42\n",
                "np.random.seed(RANDOM_SEED)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Load and Preprocess Data\n",
                "file_path = \"ecommerce_churn_llm_final.csv\"\n",
                "if not os.path.exists(file_path):\n",
                "    # Fallback to simulation if file is missing (e.g. fresh run)\n",
                "    from src.data_preparation import simulate_ecommerce_dataset\n",
                "    print(\"Dataset not found. Generating simulated dataset...\")\n",
                "    df = simulate_ecommerce_dataset(n_samples=1800)\n",
                "else:\n",
                "    df = pd.read_csv(file_path)\n",
                "\n",
                "print(f\"Initial shape: {df.shape}\")\n",
                "\n",
                "# Apply Modular Cleaning\n",
                "df_clean = clean_data(df)\n",
                "print(f\"Cleaned shape: {df_clean.shape}\")\n",
                "\n",
                "# Apply Sentiment Analysis (AI-Enhanced Feature)\n",
                "print(\"Running Sentiment Analysis. This may take a minute...\")\n",
                "df_clean = add_sentiment_features(df_clean)\n",
                "df_clean.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Exploratory Analysis Check\n",
                "plot_churn_distribution(df_clean)\n",
                "plot_correlation_heatmap(df_clean)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Data Splitting & Pipeline Setup\n",
                "drop_cols = [\"customer_id\", \"churn\", \"customer_feedback\", \"support_chat_excerpt\", \"reason_for_low_activity\", \"combined_text\", \"sentiment_label\"]\n",
                "X = df_clean.drop(columns=drop_cols, errors=\"ignore\")\n",
                "y = df_clean[\"churn\"]\n",
                "\n",
                "categorical_cols = X.select_dtypes(include=['object', 'string', 'category']).columns.tolist()\n",
                "numerical_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
                "\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', StandardScaler(), numerical_cols),\n",
                "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_cols)\n",
                "    ]\n",
                ")\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Advanced Modeling with Hyperparameter Tuning\n",
                "tuned_models = {\n",
                "    \"Logistic Regression\": (LogisticRegression(max_iter=1000, random_state=RANDOM_SEED), \n",
                "                           {'classifier__C': [0.1, 1, 10]}),\n",
                "    \"Decision Tree\": (DecisionTreeClassifier(random_state=RANDOM_SEED), \n",
                "                     {'classifier__max_depth': [5, 10, None]}),\n",
                "    \"Random Forest\": (RandomForestClassifier(random_state=RANDOM_SEED), \n",
                "                     {'classifier__n_estimators': [50, 100], 'classifier__max_depth': [10, 20]}),\n",
                "    \"XGBoost\": (XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_SEED), \n",
                "               {'classifier__n_estimators': [50, 100], 'classifier__learning_rate': [0.01, 0.1]}),\n",
                "    \"MLP NN\": (MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=RANDOM_SEED), \n",
                "              {'classifier__alpha': [0.0001, 0.001]})\n",
                "}\n",
                "\n",
                "results = []\n",
                "best_estimators = {}\n",
                "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
                "\n",
                "for name, (model, param_grid) in tuned_models.items():\n",
                "    print(f\"Tuning and evaluating {name}...\")\n",
                "    \n",
                "    pipeline = Pipeline([\n",
                "        ('preprocessor', preprocessor),\n",
                "        ('classifier', model)\n",
                "    ])\n",
                "    \n",
                "    # GridSearchCV\n",
                "    grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
                "    grid_search.fit(X_train, y_train)\n",
                "    \n",
                "    best_pipe = grid_search.best_estimator_\n",
                "    best_estimators[name] = best_pipe\n",
                "    \n",
                "    # Test Prediction\n",
                "    y_pred = best_pipe.predict(X_test)\n",
                "    y_prob = best_pipe.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    results.append({\n",
                "        \"Model\": name,\n",
                "        \"Best Params\": grid_search.best_params_,\n",
                "        \"CV ROC AUC\": grid_search.best_score_,\n",
                "        \"Test Accuracy\": accuracy_score(y_test, y_pred),\n",
                "        \"Test F1\": f1_score(y_test, y_pred),\n",
                "        \"Test ROC AUC\": roc_auc_score(y_test, y_prob)\n",
                "    })\n",
                "\n",
                "results_df = pd.DataFrame(results).sort_values(by=\"Test ROC AUC\", ascending=False)\n",
                "display(results_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Visual Evaluation & Confusion Matrices\n",
                "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, (name, pipe) in enumerate(best_estimators.items()):\n",
                "    y_pred = pipe.predict(X_test)\n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    sns.heatmap(cm, annot=True, fmt='d', ax=axes[i], cmap='Blues')\n",
                "    axes[i].set_title(f\"{name} Confusion Matrix\")\n",
                "    axes[i].set_xlabel(\"Predicted\")\n",
                "    axes[i].set_ylabel(\"Actual\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Model Explainability with SHAP (Best Model)\n",
                "best_model_name = results_df.iloc[0]['Model']\n",
                "best_pipeline = best_estimators[best_model_name]\n",
                "\n",
                "print(f\"Generating SHAP analysis for the best model: {best_model_name}\")\n",
                "\n",
                "# Extract the classifier and preprocessed data\n",
                "classifier = best_pipeline.named_steps['classifier']\n",
                "X_test_preprocessed = best_pipeline.named_steps['preprocessor'].transform(X_test)\n",
                "\n",
                "# Get feature names after OHE\n",
                "ohe_features = best_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_cols).tolist()\n",
                "all_features = numerical_cols + ohe_features\n",
                "\n",
                "# Use SHAP KernelExplainer for model-agnostic explanation (or TreeExplainer if Tree-based)\n",
                "# Note: KernelExplainer is slow, so we use a subset for demonstration\n",
                "explainer = shap.Explainer(classifier, X_test_preprocessed)\n",
                "shap_values = explainer(X_test_preprocessed[:100])\n",
                "\n",
                "shap.summary_plot(shap_values, X_test_preprocessed[:100], feature_names=all_features)"
            ]
        }\n",
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}